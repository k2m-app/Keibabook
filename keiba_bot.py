import time
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘
# ==================================================

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"

def set_race_params(year, kai, place, day):
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(year, kai, place_code, place_name, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if supabase is None:
        print("âš  Supabase æœªè¨­å®š â†’ ä¿å­˜ã—ã¾ã›ã‚“")
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
        print("ğŸ’¾ å±¥æ­´ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print("âš  å±¥æ­´ä¿å­˜å¤±æ•—:", e)


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹
# ==================================================
def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" in s)
    if not h2:
        return []

    midasi = h2.find_parent("div", class_="midasi")
    table = midasi.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return []

    rows = table.tbody.find_all("tr")
    result = []
    i = 0

    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = uma_td.get_text(strip=True)
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        result.append({
            "waku": waku,
            "umaban": umaban,
            "name": name,
            "prev_date_course": prev_date,
            "prev_class": prev_class,
            "prev_finish": prev_finish,
            "prev_comment": prev_comment,
        })

        i += 2

    return result


def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table:
        return {}

    danwa_dict = {}
    current = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        if uma_td:
            current = uma_td.get_text(strip=True)
            continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current:
            danwa_dict[current] = danwa_td.get_text(strip=True)
            current = None

    return danwa_dict


# ==================================================
# â˜… èª¿æ•™ãƒšãƒ¼ã‚¸ ãƒ‘ãƒ¼ã‚¹ï¼ˆå®Œå…¨ç‰ˆï¼‰
# ==================================================
def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    h2 = soup.find("h2", string=lambda s: s and "èª¿æ•™" in s)
    if h2:
        root = h2.find_parent("div")
        section = root.find_next_sibling("div", class_="section")
    else:
        section = soup

    if not section:
        return {}

    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue

        rows = tbody.find_all("tr", recursive=False)
        if not rows:
            continue

        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        if not uma_td:
            continue

        umaban = uma_td.get_text(strip=True)

        header_text = " ".join(
            td.get_text(" ", strip=True)
            for td in header.find_all("td")
        )

        detail_text = ""
        if len(rows) >= 2:
            detail_text = rows[1].get_text(" ", strip=True)

        final_text = " ".join([header_text, detail_text]).strip()
        cyokyo_dict[umaban] = final_text

    return cyokyo_dict


# ==================================================
# èª¿æ•™å–å¾—é–¢æ•°
# ==================================================
BASE_URL = "https://s.keibabook.co.jp"

def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except:
        return {}

    html = driver.page_source
    cy = parse_cyokyo(html)

    print("â˜…DEBUG èª¿æ•™:", cy)
    return cy


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
def run_all_races(target_races=None):

    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_names = {
        "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰",
        "04": "æ±äº¬", "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ",
        "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
    }
    place_name = place_names.get(PLACE, "ä¸æ˜")

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        # ãƒ­ã‚°ã‚¤ãƒ³
        driver.get("https://s.keibabook.co.jp/login/login")

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)

        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()

        time.sleep(2)

        # å„Rå‡¦ç†
        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            print(f"\n=== {r}R ===")

            # å©èˆã‚³ãƒ¡ãƒ³ãƒˆ
            url_danwa = f"https://s.keibabook.co.jp/cyuou/danwa/0/{race_id}"
            driver.get(url_danwa)
            time.sleep(1)

            html_danwa = driver.page_source
            danwa_dict = parse_danwa_comments(html_danwa)

            # å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
            url_inter = f"https://s.keibabook.co.jp/cyuou/syoin/{race_id}"
            driver.get(url_inter)
            time.sleep(1)
            zenkoso = parse_zenkoso_interview(driver.page_source)

            # èª¿æ•™
            cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

            # ãƒãƒ¼ã‚¸
            merged = []
            for h in zenkoso:
                uma = h["umaban"]
                text = (
                    f"â–¼[æ {h['waku']} é¦¬ç•ª{uma}] {h['name']}\n"
                    f"  ã€å©èˆã®è©±ã€‘ {danwa_dict.get(uma, 'ï¼ˆå©èˆã‚³ãƒ¡ãƒ³ãƒˆãªã—ï¼‰')}\n"
                    f"  ã€å‰èµ°æƒ…å ±ã€‘ {h['prev_date_course']} ({h['prev_class']}) {h['prev_finish']}\n"
                    f"  ã€å‰èµ°è«‡è©±ã€‘ {h['prev_comment'] or 'ï¼ˆå‰èµ°è«‡è©±ãªã—ï¼‰'}\n"
                    f"  ã€èª¿æ•™ã€‘ {cyokyo_dict.get(uma, 'ï¼ˆèª¿æ•™æƒ…å ±ãªã—ï¼‰')}\n"
                )
                merged.append(text)

            full_text = (
                f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚"
                f"å„é¦¬ã«ã¤ã„ã¦ã€å©èˆã®è©±ã€‘ã€å‰èµ°æƒ…å ±ãƒ»å‰èµ°è«‡è©±ã€‘ã€èª¿æ•™ã€‘ã‚’åŸºã«åˆ†æã›ã‚ˆã€‚\n\n"
                f"â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n" +
                "\n".join(merged)
            )

            payload = {
                "inputs": {"text": full_text},
                "response_mode": "blocking",
                "user": "keiba-bot-user",
            }

            headers = {
                "Authorization": f"Bearer {DIFY_API_KEY}",
                "Content-Type": "application/json",
            }

            res = requests.post("https://api.dify.ai/v1/workflows/run",
                                headers=headers, json=payload)

            if res.status_code == 200:
                ans = res.json().get("data", {}).get("outputs", {}).get("answer", "")
                st.markdown(f"### {place_name} {r}R")
                st.write(ans)
                st.write("---")

                save_history(YEAR, KAI, PLACE, place_name, DAY,
                             race_num, race_id, ans)

            else:
                print("âŒ Dify ã‚¨ãƒ©ãƒ¼:", res.status_code, res.text)

    finally:
        print("\nğŸ§¹ ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†")
        driver.quit()
