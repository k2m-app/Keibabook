import time
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client


# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘
# ==================================================

# 1. ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ï¼ˆSecretsã‹ã‚‰å–å¾—ï¼‰
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")

# 2. Dify APIã‚­ãƒ¼ï¼ˆSecretsã‹ã‚‰å–å¾—ï¼‰
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# 3. Supabase ã® URL ã¨ anon keyï¼ˆSecrets ã‹ã‚‰å–å¾—ï¼‰
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# 4. é–‹å‚¬æƒ…å ±ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"

def set_race_params(year, kai, place, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å—ã‘å–ã£ã¦ä¸Šæ›¸ã"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(year, kai, place_code, place_name, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if supabase is None:
        print("âš  Supabase æœªè¨­å®šã®ãŸã‚å±¥æ­´ä¿å­˜ã‚¹ã‚­ãƒƒãƒ—")
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
        print("ğŸ’¾ å±¥æ­´ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"âš  å±¥æ­´ä¿å­˜å¤±æ•—: {e}")


# ==================================================
# HTMLãƒ‘ãƒ¼ã‚¹é–¢æ•°
# ==================================================
def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" in s)
    if not h2:
        return []

    midasi_div = h2.find_parent("div", class_="midasi")
    table = midasi_div.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return []

    rows = table.tbody.find_all("tr")
    result = []
    i = 0

    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        umaban_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if not (waku_td and umaban_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = umaban_td.get_text(strip=True)
        name = bamei_td.get_text(strip=True)

        prev_date_course = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail_row = rows[i + 1] if i + 1 < len(rows) else None
        if detail_row:
            syoin_td = detail_row.find("td", class_="syoin")
            if syoin_td:
                syoindata = syoin_td.find("div", class_="syoindata")
                if syoindata:
                    ps = syoindata.find_all("p")
                    if len(ps) >= 1:
                        prev_date_course = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct_ps = syoin_td.find_all("p", recursive=False)
                if direct_ps:
                    txt = direct_ps[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        result.append({
            "waku": waku,
            "umaban": umaban,
            "name": name,
            "prev_date_course": prev_date_course,
            "prev_class": prev_class,
            "prev_finish": prev_finish,
            "prev_comment": prev_comment,
        })

        i += 2
        if i < len(rows) and "spacer" in (rows[i].get("class") or []):
            i += 1

    return result


def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table:
        return {}

    danwa_dict = {}
    rows = table.tbody.find_all("tr")
    current_umaban = None

    for row in rows:
        umaban_td = row.find("td", class_="umaban")
        if umaban_td:
            current_umaban = umaban_td.get_text(strip=True)
            continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_umaban:
            danwa_dict[current_umaban] = danwa_td.get_text(strip=True)
            current_umaban = None

    return danwa_dict


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆâ˜…ã“ã“ãŒä»Šå›ã®ä¿®æ­£ç‰ˆï¼‰
# ==================================================
def run_all_races(target_races=None):
    """
    target_races = [3, 5, 7] ã®ã‚ˆã†ã«æ¸¡ã™ã¨ã€ãã®ãƒ¬ãƒ¼ã‚¹ã ã‘å®Ÿè¡Œã€‚
    Noneï¼ˆæœªæŒ‡å®šï¼‰ã®å ´åˆã¯ 1ã€œ12R ã™ã¹ã¦å®Ÿè¡Œã€‚
    """

    # ãƒ¬ãƒ¼ã‚¹ç•ªå·ã®æ±ºå®š
    if target_races is None:
        race_numbers = list(range(1, 13))
    else:
        race_numbers = sorted({int(r) for r in target_races})

    base_race_id = f"{YEAR}{KAI}{PLACE}{DAY}"

    place_names = {
        "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰",
        "04": "æ±äº¬", "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ",
        "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
    }
    place_name = place_names.get(PLACE, "ä¸æ˜")

    print(f"ğŸ”¥ å®Ÿè¡Œãƒ¬ãƒ¼ã‚¹ï¼š{race_numbers}")

    # Selenium è¨­å®š
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=options)

    try:
        # ãƒ­ã‚°ã‚¤ãƒ³
        driver.get("https://s.keibabook.co.jp/login/login")

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)

        try:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "btn-login"))
            ).click()
        except:
            WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit']"))
            ).click()

        time.sleep(2)

        # â˜…ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¬ãƒ¼ã‚¹ã®ã¿å®Ÿè¡Œ
        for i in race_numbers:
            race_num_str = f"{i:02}"
            current_race_id = base_race_id + race_num_str

            print(f"\n=== {i}R é–‹å§‹ ===")

            # 1. å©èˆã‚³ãƒ¡ãƒ³ãƒˆãƒšãƒ¼ã‚¸
            url_danwa = f"https://s.keibabook.co.jp/cyuou/danwa/0/{current_race_id}"
            driver.get(url_danwa)
            time.sleep(1)

            if "login" in driver.current_url:
                print("âš  ãƒ­ã‚°ã‚¤ãƒ³åˆ‡ã‚Œ â†’ ã“ã®ãƒ¬ãƒ¼ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                continue

            try:
                title_block = WebDriverWait(driver, 5).until(
                    EC.visibility_of_element_located((By.CSS_SELECTOR, "div.racetitle"))
                )
                race_title = title_block.text.strip()
            except:
                race_title = f"{place_name} {i}R"

            html_danwa = driver.page_source
            danwa_data = parse_danwa_comments(html_danwa)

            # 2. å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
            url_interview = f"https://s.keibabook.co.jp/cyuou/syoin/{current_race_id}"
            driver.get(url_interview)
            time.sleep(1)

            html_interview = driver.page_source
            zenkoso_list = parse_zenkoso_interview(html_interview)

            # 3. ãƒãƒ¼ã‚¸
            merged_lines = []

            if not zenkoso_list:
                merged_lines.append("ï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ï¼‰")
            else:
                for horse in zenkoso_list:
                    umaban = horse["umaban"]
                    name = horse["name"]
                    danwa = danwa_data.get(umaban, "ï¼ˆå©èˆã‚³ãƒ¡ãƒ³ãƒˆãªã—ï¼‰")

                    if horse["prev_date_course"]:
                        prev_info = f"{horse['prev_date_course']} ({horse['prev_class']}) {horse['prev_finish']}"
                    else:
                        prev_info = "ï¼ˆå‰èµ°æƒ…å ±ãªã—ï¼‰"

                    prev_comment = horse["prev_comment"] or "ï¼ˆå‰èµ°è«‡è©±ãªã—ï¼‰"

                    block = (
                        f"â–¼[æ {horse['waku']} é¦¬ç•ª{umaban}] {name}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {danwa}\n"
                        f"  ã€å‰èµ°æƒ…å ±ã€‘ {prev_info}\n"
                        f"  ã€å‰èµ°è«‡è©±ã€‘ {prev_comment}\n"
                    )
                    merged_lines.append(block)

            full_text = (
                f"ã‚ãªãŸã¯ãƒ—ãƒ­ã®ç«¶é¦¬äºˆæƒ³AIã§ã™ã€‚ä»¥ä¸‹ã®{place_name}{i}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€"
                f"æ¨å¥¨é¦¬ã¨ãã®æ ¹æ‹ ã€å±•é–‹äºˆæƒ³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\n"
                f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{race_title}\n\n"
                f"â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n" +
                "\n".join(merged_lines)
            )

            # 4. Dify API å‘¼ã³å‡ºã—
            payload = {
                "inputs": {"text": full_text},
                "response_mode": "blocking",
                "user": "keiba-bot-user",
            }

            headers = {
                "Authorization": f"Bearer {DIFY_API_KEY}",
                "Content-Type": "application/json",
            }

            res = requests.post("https://api.dify.ai/v1/workflows/run",
                                headers=headers, json=payload)

            if res.status_code == 200:
                data = res.json()
                ai_answer = (
                    data.get("data", {})
                        .get("outputs", {})
                        .get("answer", "")
                )

                st.markdown(f"### {place_name} {i}R")
                st.write(ai_answer)
                st.write("---")

                save_history(YEAR, KAI, PLACE, place_name, DAY,
                             race_num_str, current_race_id, ai_answer)
            else:
                print(f"âŒ Dify ã‚¨ãƒ©ãƒ¼: {res.status_code} {res.text}")

    finally:
        print("\nğŸ§¹ ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†")
        driver.quit()
