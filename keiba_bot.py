import time
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘
# ==================================================

# 1. ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ï¼ˆSecretsã‹ã‚‰å–å¾—ï¼‰
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")

# 2. Dify APIã‚­ãƒ¼ï¼ˆSecretsã‹ã‚‰å–å¾—ï¼‰
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# 3. Supabase ã® URL ã¨ anon keyï¼ˆSecrets ã‹ã‚‰å–å¾—ï¼‰
SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# 4. é–‹å‚¬æƒ…å ±ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
# å¿…è¦ã«å¿œã˜ã¦ set_race_params ã§æ›¸ãæ›ãˆã¦ãã ã•ã„
YEAR = "2025"
KAI = "04"
PLACE = "02" # 02:ä¸­äº¬
DAY = "02"   # 2æ—¥ç›® (ä¾‹ã¨ã—ã¦å¤‰æ›´)


# ==================================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£é–¢æ•° (Supabase)
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client:
    """Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’1å›ã ã‘ä½œã£ã¦ä½¿ã„å›ã™"""
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(year, kai, place_code, place_name, day, race_num_str, race_id, ai_answer):
    """1ãƒ¬ãƒ¼ã‚¹åˆ†ã®AIå‡ºåŠ›ã‚’ Supabase ã® history ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹"""
    supabase = get_supabase_client()
    if supabase is None:
        print("âš  Supabase æœªè¨­å®šã®ãŸã‚å±¥æ­´ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
        print("ğŸ’¾ å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âš  å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


# ==================================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ»ãƒ‘ãƒ¼ã‚¹é–¢é€£é–¢æ•°
# ==================================================

def parse_zenkoso_interview(html: str):
    """
    å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒšãƒ¼ã‚¸ã®HTMLã‹ã‚‰ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹
    """
    soup = BeautifulSoup(html, "html.parser")
    # ã‚¿ã‚¤ãƒˆãƒ«å‘¨è¾ºã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" in s)
    if not h2:
        return []

    midasi_div = h2.find_parent("div", class_="midasi")
    table = midasi_div.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return []

    rows = table.tbody.find_all("tr")
    result = []
    i = 0
    while i < len(rows):
        row = rows[i]
        
        # spacer è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        # æ ãƒ»é¦¬ç•ªãƒ»é¦¬åè¡Œã‚’åˆ¤å®š
        waku_td = row.find("td", class_="waku")
        umaban_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        
        if not (waku_td and umaban_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = umaban_td.get_text(strip=True)
        name = bamei_td.get_text(strip=True)

        # æ¬¡ã®è¡ŒãŒè©³ç´°æƒ…å ±
        detail_row = rows[i + 1] if i + 1 < len(rows) else None
        prev_date_course = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        if detail_row:
            syoin_td = detail_row.find("td", class_="syoin")
            if syoin_td:
                # å‰èµ°ã®æ—¥ä»˜ï¼‹ã‚³ãƒ¼ã‚¹ãªã©
                syoindata = syoin_td.find("div", class_="syoindata")
                if syoindata:
                    ps = syoindata.find_all("p")
                    if len(ps) >= 1:
                        prev_date_course = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                # ã‚³ãƒ¡ãƒ³ãƒˆ
                direct_ps = syoin_td.find_all("p", recursive=False)
                if direct_ps:
                    comment_text = direct_ps[0].get_text(strip=True)
                    if comment_text != "ï¼":
                        prev_comment = comment_text.lstrip("ã€€ ").rstrip()

        result.append({
            "waku": waku,
            "umaban": umaban,
            "name": name,
            "prev_date_course": prev_date_course,
            "prev_class": prev_class,
            "prev_finish": prev_finish,
            "prev_comment": prev_comment,
        })
        
        # æ¬¡ã®é¦¬ã¸é€²ã‚ã‚‹ï¼ˆé¦¬æƒ…å ±ã®è¡Œ + è©³ç´°è¡Œ + spacerè¡ŒãŒã‚ã‚‹ã‹ã‚‚ï¼‰
        i += 2
        if i < len(rows) and "spacer" in (rows[i].get("class") or []):
            i += 1

    return result


def parse_danwa_comments(html: str):
    """
    ã€æ–°è¦è¿½åŠ ã€‘å©èˆã®è©±ãƒšãƒ¼ã‚¸ã‹ã‚‰é¦¬ã”ã¨ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¾æ›¸å½¢å¼ã§æŠ½å‡ºã™ã‚‹
    Key: é¦¬ç•ª(str), Value: ã‚³ãƒ¡ãƒ³ãƒˆ(str)
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table:
        return {}

    danwa_dict = {}
    rows = table.tbody.find_all("tr")
    
    current_umaban = None
    
    for row in rows:
        # 1. é¦¬ç•ªãƒ»é¦¬åã®è¡Œã‚’æ¢ã™
        umaban_td = row.find("td", class_="umaban")
        if umaban_td:
            current_umaban = umaban_td.get_text(strip=True)
            continue
            
        # 2. ã‚³ãƒ¡ãƒ³ãƒˆã®è¡Œã‚’æ¢ã™ï¼ˆé¦¬ç•ªè¡Œã®ç›´å¾Œã«æ¥ã‚‹ï¼‰
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_umaban:
            comment = danwa_td.get_text(strip=True)
            danwa_dict[current_umaban] = comment
            current_umaban = None # æ¬¡ã®ãŸã‚ã«ãƒªã‚»ãƒƒãƒˆ

    return danwa_dict


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
def run_all_races():
    base_race_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_names = {
        "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰",
        "04": "æ±äº¬", "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ",
        "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
    }
    place_name = place_names.get(PLACE, "ä¸æ˜ãªç«¶é¦¬å ´")

    print(f"ğŸš€ {YEAR}å¹´{KAI}å› {place_name} {DAY}æ—¥ç›®ã®å…¨ãƒ¬ãƒ¼ã‚¹æ”»ç•¥ã‚’é–‹å§‹ã—ã¾ã™ï¼")

    # â–¼â–¼ ã‚¯ãƒ©ã‚¦ãƒ‰ç”¨è¨­å®šï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼‰ â–¼â–¼
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=options)

    try:
        # --- ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç† ---
        print("ğŸŒ ç«¶é¦¬ãƒ–ãƒƒã‚¯ã«ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã¸ç§»å‹•ä¸­...")
        driver.get("https://s.keibabook.co.jp/login/login")

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)
        time.sleep(0.5)

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)
        time.sleep(0.5)

        try:
            WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "btn-login"))
            ).click()
        except:
            WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit']"))
            ).click()

        print("âœ¨ ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†å®Œäº†")
        time.sleep(3)

        # --- 1Rã‹ã‚‰12Rã¾ã§ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
        for i in range(1, 13):
            race_num_str = f"{i:02}"
            current_race_id = base_race_id + race_num_str

            print("\n" + "=" * 40)
            print(f"ğŸ {place_name} {i}R (ID:{current_race_id}) ã®æƒ…å ±ã‚’åé›†ä¸­...")

            try:
                url_danwa = f"https://s.keibabook.co.jp/cyuou/danwa/0/{current_race_id}"
                url_interview = f"https://s.keibabook.co.jp/cyuou/syoin/{current_race_id}"

                # -------------------------------------------------------
                # 1. å©èˆã®è©±ãƒšãƒ¼ã‚¸å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹
                # -------------------------------------------------------
                driver.get(url_danwa)
                time.sleep(1)

                if "login" in driver.current_url:
                    print("âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¤–ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                # ãƒ¬ãƒ¼ã‚¹åå–å¾—
                try:
                    race_title_block = WebDriverWait(driver, 5).until(
                        EC.visibility_of_element_located((By.CSS_SELECTOR, "div.racetitle"))
                    )
                    race_title_text = race_title_block.text.strip()
                except:
                    race_title_text = f"{place_name} {i}R"

                # HTMLã‹ã‚‰å©èˆã‚³ãƒ¡ãƒ³ãƒˆã‚’è¾æ›¸åŒ–
                html_danwa = driver.page_source
                danwa_data = parse_danwa_comments(html_danwa)

                # -------------------------------------------------------
                # 2. å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒšãƒ¼ã‚¸å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹
                # -------------------------------------------------------
                driver.get(url_interview)
                time.sleep(1)
                
                html_interview = driver.page_source
                zenkoso_list = parse_zenkoso_interview(html_interview)

                # -------------------------------------------------------
                # 3. ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œé¦¬ã”ã¨ã€ã«ãƒãƒ¼ã‚¸ã—ã¦æ§‹é€ åŒ–ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
                # -------------------------------------------------------
                merged_lines = []
                
                if not zenkoso_list:
                    # å‰èµ°æƒ…å ±ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆï¼ˆæ–°é¦¬æˆ¦ãªã©ï¼‰ã®ã‚¬ãƒ¼ãƒ‰
                    merged_lines.append("ï¼ˆå‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼‰")
                else:
                    for horse in zenkoso_list:
                        umaban = horse['umaban']
                        name = horse['name']
                        
                        # å©èˆã‚³ãƒ¡ãƒ³ãƒˆã‚’è¾æ›¸ã‹ã‚‰å¼•ãï¼ˆãªã‘ã‚Œã°ã€Œãªã—ã€ï¼‰
                        danwa_comment = danwa_data.get(umaban, "ï¼ˆå©èˆã‚³ãƒ¡ãƒ³ãƒˆãªã—ï¼‰")
                        
                        # å‰èµ°æƒ…å ±ã®æ•´å½¢
                        if horse['prev_date_course']:
                            prev_info = f"{horse['prev_date_course']} ({horse['prev_class']}) {horse['prev_finish']}"
                        else:
                            prev_info = "ï¼ˆå‰èµ°æƒ…å ±ãªã—ï¼‰"
                            
                        prev_comment = horse['prev_comment'] or "ï¼ˆå‰èµ°ã‚³ãƒ¡ãƒ³ãƒˆãªã—ï¼‰"

                        # 1é ­åˆ†ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆ
                        block = (
                            f"â–¼[æ {horse['waku']} é¦¬ç•ª{umaban}] {name}\n"
                            f"  ã€å©èˆã®è©±ã€‘ {danwa_comment}\n"
                            f"  ã€å‰èµ°æƒ…å ±ã€‘ {prev_info}\n"
                            f"  ã€å‰èµ°è«‡è©±ã€‘ {prev_comment}\n"
                        )
                        merged_lines.append(block)

                # æœ€çµ‚çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
                full_text = (
                    f"ã‚ãªãŸã¯ãƒ—ãƒ­ã®ç«¶é¦¬äºˆæƒ³AIã§ã™ã€‚ä»¥ä¸‹ã®{place_name}{i}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€"
                    f"æ¨å¥¨é¦¬ã¨ãã®æ ¹æ‹ ã€å±•é–‹äºˆæƒ³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\n"
                    f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n{race_title_text}\n\n"
                    f"â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨é ­åˆ†ï¼‰\n"
                    + "\n".join(merged_lines)
                )

                # -------------------------------------------------------
                # 4. Difyã«åˆ†æã•ã›ã‚‹
                # -------------------------------------------------------
                print(f"ğŸ§  {place_name} {i}Rã‚’åˆ†æä¸­...")

                url = "https://api.dify.ai/v1/workflows/run"
                headers = {
                    "Authorization": f"Bearer {DIFY_API_KEY}",
                    "Content-Type": "application/json",
                }
                payload = {
                    "inputs": {"text": full_text},
                    "response_mode": "blocking",
                    "user": "keiba-bot-user",
                }

                response = requests.post(url, headers=headers, json=payload)

                if response.status_code == 200:
                    result = response.json()
                    outputs = result.get("data", {}).get("outputs") or result.get("data") or {}
                    ai_answer = outputs.get("answer")

                    if ai_answer:
                        print(f"ğŸ¯ {place_name} {i}R åˆ†æå®Œäº†ï¼ˆä¿å­˜ã—ã¾ã™ï¼‰")
                        
                        # Streamlitç”»é¢è¡¨ç¤º
                        st.markdown(f"### {place_name} {i}R")
                        st.write(ai_answer)
                        st.write("---")

                        # Supabaseã¸ä¿å­˜
                        save_history(
                            YEAR, KAI, PLACE, place_name, DAY,
                            race_num_str, current_race_id, ai_answer
                        )
                    else:
                        print("âš ï¸ åˆ†æçµæœãŒç©ºã§ã—ãŸã€‚")
                else:
                    print(f"âŒ Difyé€šä¿¡ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")

            except Exception as e:
                print(f"âŒ {i}Rå‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")

    finally:
        print("\nğŸ§¹ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã™")
        driver.quit()


if __name__ == "__main__":
    # Streamlitã§èµ·å‹•ã™ã‚‹å ´åˆã€ãƒœã‚¿ãƒ³ãªã©ã§ç™ºç«ã•ã›ã‚‹ã¨ç®¡ç†ã—ã‚„ã™ã„ã§ã™ãŒ
    # ã“ã“ã§ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã«å³èµ°ã‚‹æ§‹æˆã«ã—ã¦ã„ã¾ã™
    run_all_races()
