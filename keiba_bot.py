import time
import json
import re
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆapp.py å´ã§ set_race_params ãŒå‘¼ã°ã‚Œã‚‹ã¨æ›¸ãæ›ã‚ã‚‹ï¼‰
YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"

BASE_URL = "https://s.keibabook.co.jp"

PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}


def set_race_params(year, kai, place, day):
    """app.py ã‹ã‚‰é–‹å‚¬æƒ…å ±ã‚’å·®ã—æ›¿ãˆã‚‹ãŸã‚ã®é–¢æ•°"""
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


def get_current_params():
    """ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆUIè¡¨ç¤ºç”¨ï¼‰"""
    return YEAR, KAI, PLACE, DAY


# ==================================================
# Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client | None:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(
    year: str,
    kai: str,
    place_code: str,
    place_name: str,
    day: str,
    race_num_str: str,
    race_id: str,
    ai_answer: str,
) -> None:
    """history ãƒ†ãƒ¼ãƒ–ãƒ«ã« 1 ãƒ¬ãƒ¼ã‚¹åˆ†ã®äºˆæƒ³ã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    supabase = get_supabase_client()
    if supabase is None:
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
    except Exception as e:
        print("Supabase insert error:", e)


# ==================================================
# Seleniumï¼ˆChromeï¼‰ç”Ÿæˆ
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")  # æ–°ã—ã‚ã® headless
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2000")

    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver


def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒ secrets ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    driver.get(f"{BASE_URL}/login/login")

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.NAME, "login_id"))
    ).send_keys(KEIBA_ID)

    WebDriverWait(driver, 15).until(
        EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
    ).send_keys(KEIBA_PASS)

    WebDriverWait(driver, 15).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
    ).click()

    time.sleep(1.5)


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹é–¢æ•°ç¾¤
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}

    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2:
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


def parse_zenkoso_interview(html: str):
    """
    å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã‚’å–å¾—ã€‚æ–°é¦¬æˆ¦ãªã©ã§å­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºè¾æ›¸ã‚’è¿”ã™ã€‚
    æˆ»ã‚Šå€¤ï¼š{ "1": {...}, "2": {...} } ï¼ˆé¦¬ç•ªã‚­ãƒ¼ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2:
        return {}

    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return {}

    rows = table.tbody.find_all("tr")
    result_dict = {}

    i = 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        if umaban:
            result_dict[umaban] = {
                "waku": waku,
                "umaban": umaban,
                "name": name,
                "prev_date_course": prev_date,
                "prev_class": prev_class,
                "prev_finish": prev_finish,
                "prev_comment": prev_comment,
            }

        i += 2

    return result_dict


def parse_danwa_comments(html: str):
    """
    å©èˆã®è©±ã‚’å–å¾—ã€‚
    æˆ»ã‚Šå€¤ï¼š{ "1": "ã‚³ãƒ¡ãƒ³ãƒˆ", ... }ï¼ˆé¦¬ç•ªå„ªå…ˆã€‚ç„¡ç†ãªã‚‰é¦¬åã‚­ãƒ¼ã‚‚æ··åœ¨ã—å¾—ã‚‹ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody:
        return {}

    danwa_dict = {}
    current_key = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")

        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text:
                current_key = text
                continue

        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text:
                current_key = text
                continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None

    return danwa_dict


def parse_cyokyo(html: str):
    """
    èª¿æ•™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€‚
    æˆ»ã‚Šå€¤ï¼š
      - é¦¬ç•ªã‚­ãƒ¼: { "1": {"tanpyo":"", "detail":"" , "bamei_hint":""}, ... }
      - é¦¬ç•ªãŒå–ã‚Œãªã„å ´åˆï¼šé¦¬åã‚­ãƒ¼ã§å…¥ã‚‹ã“ã¨ãŒã‚ã‚‹ï¼ˆæ•‘æ¸ˆç”¨ï¼‰
    """
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")
    if section is None:
        section = soup

    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1:
            continue

        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")

        umaban_text = uma_td.get_text(strip=True) if uma_td else ""
        umaban = re.sub(r"\D", "", umaban_text)

        bamei_hint = ""
        if name_td:
            bamei_hint = name_td.get_text(" ", strip=True)

        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""

        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""

        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}

        if umaban:
            cyokyo_dict[umaban] = payload
        else:
            if bamei_hint:
                cyokyo_dict[bamei_hint] = payload

    return cyokyo_dict


def parse_syutuba(html: str) -> dict:
    """
    ç¢ºå®šå‡ºé¦¬(å‡ºé¦¬è¡¨)ãƒšãƒ¼ã‚¸ã‹ã‚‰
    { "1": {"umaban":"1","bamei":"ã‚±ã‚¤ãƒ™ã‚¨","kisyu":"æœ¨å¹¡å·§"}, ... }
    ã‚’è¿”ã™ã€‚é¦¬ç•ªã‚’ä¸»ã‚­ãƒ¼ã«ã™ã‚‹ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")

    table = soup.find("table", class_=lambda c: c and "syutuba_sp" in c.split())
    if not table:
        table = soup.find("table", class_=lambda c: c and "syutuba" in c)

    if not table or not table.tbody:
        return {}

    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds:
            continue

        umaban_raw = tds[0].get_text(strip=True)
        umaban = re.sub(r"\D", "", umaban_raw)
        if not umaban:
            continue

        bamei = ""
        kbamei_p = tr.find("p", class_="kbamei")
        if kbamei_p:
            bamei = kbamei_p.get_text(" ", strip=True)

        kisyu = ""
        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p:
            a = kisyu_p.find("a")
            if a:
                kisyu = a.get_text(strip=True)

        result[umaban] = {"umaban": umaban, "bamei": bamei, "kisyu": kisyu}

    return result


# ==================================================
# fetch é–¢æ•°ç¾¤ï¼ˆSeleniumï¼‰
# ==================================================
def fetch_danwa_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/danwa/0/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)


def fetch_zenkoso_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syoin/{race_id}"
    driver.get(url)
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)


def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception:
        pass
    return parse_cyokyo(driver.page_source)


def fetch_syutuba_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/syutuba/{race_id}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.syutuba_sp, table.syutuba"))
        )
    except Exception:
        pass
    return parse_syutuba(driver.page_source)


# ==================================================
# racekey è‡ªå‹•æ¤œå‡ºï¼ˆãƒ­ã‚°ã‚¤ãƒ³å¾Œï¼‰
# ==================================================
def detect_latest_racekey(driver) -> str | None:
    """
    Keibabookå†…ãƒšãƒ¼ã‚¸ã‹ã‚‰ /cyuou/syutuba/XXXXXXXXXXXX ã‚’æ‹¾ã£ã¦æœ€æ–°ã£ã½ã„ã‚‚ã®ã‚’è¿”ã™ã€‚
    å–ã‚Œãªã‘ã‚Œã° Noneã€‚
    """
    # ã¾ãšä¸­å¤®ãƒˆãƒƒãƒ—
    driver.get(f"{BASE_URL}/cyuou/")
    time.sleep(1.0)
    html = driver.page_source

    keys = re.findall(r"/cyuou/syutuba/(\d{12})", html)
    if not keys:
        keys = re.findall(r"/cyuou/thursday/(\d{12})", html)

    if not keys:
        # è¿½åŠ ã®ä¿é™ºï¼šãƒ­ã‚°ã‚¤ãƒ³å¾Œãƒ›ãƒ¼ãƒ 
        driver.get(f"{BASE_URL}/")
        time.sleep(1.0)
        html2 = driver.page_source
        keys = re.findall(r"/cyuou/syutuba/(\d{12})", html2)
        if not keys:
            keys = re.findall(r"/cyuou/thursday/(\d{12})", html2)

    if not keys:
        return None

    # åŒæ—¥é–‹å‚¬ãªã‚‰å¤§æŠµ max ã§OKï¼ˆYYYYKAI... ã®æ•°å€¤ã¨ã—ã¦å¤§ãã„ï¼å¾Œï¼‰
    return max(keys)


def set_params_from_racekey(racekey12: str):
    """
    racekey: YYYY(4) + KAI(2) + PLACE(2) + DAY(2) + RACE(2)
    """
    year = racekey12[0:4]
    kai = racekey12[4:6]
    place = racekey12[6:8]
    day = racekey12[8:10]
    set_race_params(year, kai, place, day)


# ==================================================
# Dify ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return

    payload = {
        "inputs": {"text": full_text},
        "response_mode": "streaming",
        "user": "keiba-bot-user",
    }

    headers = {
        "Authorization": f"Bearer {DIFY_API_KEY}",
        "Content-Type": "application/json",
    }

    try:
        res = requests.post(
            "https://api.dify.ai/v1/workflows/run",
            headers=headers,
            json=payload,
            stream=True,
            timeout=300,
        )

        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: Dify API Error {res.status_code}\n{res.text}"
            return

        for line in res.iter_lines():
            if not line:
                continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"):
                continue

            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                continue

            event = data.get("event")

            if event in ["workflow_started", "node_started", "node_finished"]:
                continue

            chunk = data.get("answer", "")
            if chunk:
                yield chunk

            if event == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                if outputs:
                    found_text = ""
                    for _, value in outputs.items():
                        if isinstance(value, str):
                            found_text += value + "\n"
                    if found_text.strip():
                        yield found_text.strip()
    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"


# ==================================================
# ä¾¿åˆ©ï¼šè¾æ›¸ã®é¦¬åã‚­ãƒ¼æ•‘æ¸ˆ
# ==================================================
def _find_by_name_key(d: dict, bamei: str):
    if not bamei:
        return None
    if bamei in d:
        return d[bamei]
    # å®Œå…¨ä¸€è‡´ã ã‘ï¼ˆæ›–æ˜§ä¸€è‡´ã¯äº‹æ•…æºã«ãªã‚‹ã®ã§æŠ‘åˆ¶ï¼‰
    for k, v in d.items():
        if (not str(k).isdigit()) and (str(k).strip() == bamei.strip()):
            return v
    return None


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆè¤‡æ•°ãƒ¬ãƒ¼ã‚¹ï¼‰
# ==================================================
def run_all_races(target_races=None):
    """
    target_races: None -> 1~12
                 list/set -> æŒ‡å®šãƒ¬ãƒ¼ã‚¹ç•ªå·ã ã‘å®Ÿè¡Œ
    """
    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = PLACE_NAMES.get(PLACE, "ä¸æ˜")

    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        login_keibabook(driver)
        st.success("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            st.markdown(f"### {place_name} {r}R")
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

                # A-1 danwa + race_info
                html_danwa, race_info, danwa_dict = fetch_danwa_dict(driver, race_id)

                # A-2 syoin
                zenkoso_dict = fetch_zenkoso_dict(driver, race_id)

                # A-3 cyokyo
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

                # A-3.5 syutubaï¼ˆé¦¬ç•ªãƒ»é¦¬åãƒ»é¨æ‰‹ï¼‰â€»å…¨é ­ä¿è¨¼ã®åŸºç¤
                syutuba_dict = fetch_syutuba_dict(driver, race_id)

                if not syutuba_dict:
                    status_area.warning("âš ï¸ å‡ºé¦¬è¡¨ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆå…¨é ­ä¿è¨¼ã§ããªã„å¯èƒ½æ€§ï¼‰ã€‚")

                # A-4 çµåˆï¼ˆå‡ºé¦¬è¡¨ãƒ™ãƒ¼ã‚¹ã§å…¨é ­ä¿è¨¼ï¼‰
                merged = []
                umaban_list = (
                    sorted(syutuba_dict.keys(), key=lambda x: int(x))
                    if syutuba_dict
                    else sorted(
                        list(set(danwa_dict.keys()) | set(zenkoso_dict.keys()) | set(cyokyo_dict.keys())),
                        key=lambda x: int(x) if str(x).isdigit() else 999
                    )
                )

                for umaban in umaban_list:
                    sb = syutuba_dict.get(umaban, {})
                    bamei = (sb.get("bamei") or "").strip() or "åç§°ä¸æ˜"
                    kisyu = (sb.get("kisyu") or "").strip() or "ï¼ˆé¨æ‰‹ä¸æ˜ï¼‰"

                    # å©èˆã®è©±
                    d_comment = danwa_dict.get(umaban)
                    if not d_comment:
                        alt = _find_by_name_key(danwa_dict, bamei)
                        d_comment = alt if isinstance(alt, str) else None
                    if not d_comment:
                        d_comment = "ï¼ˆæƒ…å ±ãªã—ï¼‰"

                    # å‰èµ°
                    z_data = zenkoso_dict.get(umaban)
                    if not z_data:
                        alt = _find_by_name_key(zenkoso_dict, bamei)
                        z_data = alt if isinstance(alt, dict) else None
                    z_data = z_data or {}

                    z_prev_info = ""
                    z_comment = ""
                    if z_data:
                        z_prev_info = f"{z_data.get('prev_date_course','')} {z_data.get('prev_class','')} {z_data.get('prev_finish','')}".strip()
                        z_comment = (z_data.get("prev_comment") or "").strip()

                    if z_prev_info or z_comment:
                        prev_block = (
                            f"  ã€å‰èµ°æƒ…å ±ã€‘ {z_prev_info or 'ï¼ˆæƒ…å ±ãªã—ï¼‰'}\n"
                            f"  ã€å‰èµ°è«‡è©±ã€‘ {z_comment or 'ï¼ˆç„¡ã—ï¼‰'}\n"
                        )
                    else:
                        prev_block = "  ã€å‰èµ°ã€‘ æ–°é¦¬ï¼ˆå‰èµ°æƒ…å ±ãªã—ï¼‰\n"

                    # èª¿æ•™ï¼ˆçŸ­è©•ï¼‹è©³ç´°ã®ã¿ï¼‰
                    c = cyokyo_dict.get(umaban)
                    if not c:
                        c = _find_by_name_key(cyokyo_dict, bamei)
                    c = c or {}

                    c_tanpyo = (c.get("tanpyo") or "").strip()
                    c_detail = (c.get("detail") or "").strip()

                    if c_tanpyo or c_detail:
                        cyokyo_block = f"  ã€èª¿æ•™ã€‘ çŸ­è©•:{c_tanpyo or 'ï¼ˆãªã—ï¼‰'} / è©³ç´°:{c_detail or 'ï¼ˆãªã—ï¼‰'}\n"
                    else:
                        cyokyo_block = "  ã€èª¿æ•™ã€‘ ï¼ˆæƒ…å ±ãªã—ï¼‰\n"

                    text = (
                        f"â–¼[é¦¬ç•ª{umaban}] {bamei} / é¨æ‰‹:{kisyu}\n"
                        f"  ã€å©èˆã®è©±ã€‘ {d_comment}\n"
                        f"{prev_block}"
                        f"{cyokyo_block}"
                    )
                    merged.append(text)

                if not merged:
                    status_area.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    st.write("---")
                    continue

                # ãƒ¬ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆdanwaãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºï¼‰
                race_header_lines = []
                if race_info.get("date_meet"):
                    race_header_lines.append(race_info["date_meet"])
                if race_info.get("race_name"):
                    race_header_lines.append(race_info["race_name"])
                if race_info.get("cond1"):
                    race_header_lines.append(race_info["cond1"])
                if race_info.get("course_line"):
                    race_header_lines.append(race_info["course_line"])
                race_header = "\n".join(race_header_lines)

                merged_text = "\n".join(merged)

                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                    f"{race_header}\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚\n"
                    "å…¥åŠ›ã«å«ã¾ã‚Œã‚‹äº‹å®Ÿã®ã¿ï¼ˆé¨æ‰‹/å©èˆã®è©±/å‰èµ°è«‡è©±(ã‚ã‚Œã°)/èª¿æ•™ï¼‰ã‚’ç”¨ã„ã€æ¨æ¸¬ã¯ã—ãªã„ã€‚\n"
                    "å‡ºåŠ›ã¯å¿…ãšå…¨é ­åˆ†ã€‚é¦¬ç•ªã‚’ã‚­ãƒ¼ã«ã—ã€æ¬ æã¯ï¼ˆæƒ…å ±ãªã—ï¼‰ç­‰ã§æ˜ç¤ºã€‚\n\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n"
                    + merged_text
                )

                status_area.info("ğŸ¤– AIãŒåˆ†æãƒ»åŸ·ç­†ä¸­ã§ã™...")

                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")

                result_area.markdown(full_answer)

                if full_answer.strip():
                    status_area.success("âœ… åˆ†æå®Œäº†")
                    save_history(YEAR, KAI, PLACE, place_name, DAY, race_num, race_id, full_answer)
                else:
                    status_area.error("âš ï¸ AIã‹ã‚‰ã®å›ç­”ãŒç©ºã§ã—ãŸã€‚")

            except Exception as e:
                err_msg = f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ ({place_name} {r}R): {str(e)}"
                print(err_msg)
                status_area.error(err_msg)

            st.write("---")

    finally:
        try:
            driver.quit()
        except Exception:
            pass


# ==================================================
# è‡ªå‹•é–‹å‚¬æ¤œå‡ºï¼ˆUIã‹ã‚‰å‘¼ã¶ç”¨ï¼‰
# ==================================================
def auto_detect_meet_params() -> tuple[str, str, str, str] | None:
    """
    ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦æœ€æ–° racekey ã‚’æ‹¾ã„ã€YEAR/KAI/PLACE/DAY ã‚’è¿”ã™ã€‚
    å¤±æ•—ã—ãŸã‚‰ Noneã€‚
    """
    driver = build_driver()
    try:
        login_keibabook(driver)
        racekey = detect_latest_racekey(driver)
        if not racekey:
            return None
        year = racekey[0:4]
        kai = racekey[4:6]
        place = racekey[6:8]
        day = racekey[8:10]
        return year, kai, place, day
    finally:
        try:
            driver.quit()
        except Exception:
            pass
