import time
import json
import re
import math
import requests
import streamlit as st
import streamlit.components.v1 as components
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘secretsã‹ã‚‰èª­ã¿è¾¼ã¿
# ==================================================
KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
YEAR = "2026"
KAI = "01"
PLACE = "06" # ä¸­å±±
DAY = "05"

BASE_URL = "https://s.keibabook.co.jp"

# Keibabookã®å ´æ‰€ã‚³ãƒ¼ãƒ‰
KB_PLACE_NAMES = {
    "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰", "04": "æ±äº¬",
    "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ", "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
}

# Keibabook -> Netkeiba å ´æ‰€ã‚³ãƒ¼ãƒ‰å¤‰æ›ãƒãƒƒãƒ—
# KB: 00äº¬éƒ½, 01é˜ªç¥, 02ä¸­äº¬, 03å°å€‰, 04æ±äº¬, 05ä¸­å±±, 06ç¦å³¶, 07æ–°æ½Ÿ, 08æœ­å¹Œ, 09å‡½é¤¨
# NK: 01æœ­å¹Œ, 02å‡½é¤¨, 03ç¦å³¶, 04æ–°æ½Ÿ, 05æ±äº¬, 06ä¸­å±±, 07ä¸­äº¬, 08äº¬éƒ½, 09é˜ªç¥, 10å°å€‰
KB_TO_NK_PLACE = {
    "00": "08", "01": "09", "02": "07", "03": "10", "04": "05",
    "05": "06", "06": "03", "07": "04", "08": "01", "09": "02"
}

def set_race_params(year, kai, place, day):
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)

def get_current_params():
    return YEAR, KAI, PLACE, DAY

# ==================================================
# ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚³ãƒ”ãƒ¼
# ==================================================
def render_copy_button(text: str, label: str, dom_id: str):
    safe_text = json.dumps(text)
    html = f"""
    <div style="display:flex; gap:8px; align-items:center; flex-wrap:wrap;">
      <button id="{dom_id}" style="
        padding:8px 12px;
        border-radius:10px;
        border:1px solid #ddd;
        background:#fff;
        cursor:pointer;
        font-size:14px;
      ">{label}</button>
      <span id="{dom_id}-msg" style="font-size:12px; color:#666;"></span>
    </div>
    <script>
      (function() {{
        const btn = document.getElementById("{dom_id}");
        const msg = document.getElementById("{dom_id}-msg");
        if (!btn) return;
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({safe_text});
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(() => msg.textContent = "", 1200);
          }} catch (e) {{
            msg.textContent = "ã‚³ãƒ”ãƒ¼ã«å¤±æ•—";
            setTimeout(() => msg.textContent = "", 2200);
          }}
        }});
      }})();
    </script>
    """
    components.html(html, height=54)


# ==================================================
# Selenium
# ==================================================
def build_driver() -> webdriver.Chrome:
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1280,2200")
    # User-Agentè¨­å®šï¼ˆNetkeibaå¯¾ç­–ï¼‰
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(60)
    return driver

def login_keibabook(driver: webdriver.Chrome) -> None:
    if not KEIBA_ID or not KEIBA_PASS:
        raise RuntimeError("KEIBA_ID / KEIBA_PASS ãŒæœªè¨­å®š")
    driver.get(f"{BASE_URL}/login/login")
    WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.NAME, "login_id"))).send_keys(KEIBA_ID)
    WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))).send_keys(KEIBA_PASS)
    WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))).click()
    time.sleep(1.2)

# ==================================================
# Keibabook Parser
# ==================================================
def parse_race_info(html: str):
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {"date_meet": "", "race_name": "", "cond1": "", "course_line": ""}
    racemei = racetitle.find("div", class_="racemei")
    date_meet, race_name = "", ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1: date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2: race_name = ps[1].get_text(strip=True)
    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1, course_line = "", ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1: cond1 = sub_ps[0].get_text(strip=True)
        if len(sub_ps) >= 2: course_line = sub_ps[1].get_text(" ", strip=True)
    return {"date_meet": date_meet, "race_name": race_name, "cond1": cond1, "course_line": course_line}

def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table or not table.tbody: return {}
    danwa_dict, current_key = {}, None
    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if uma_td:
            text = re.sub(r"\D", "", uma_td.get_text(strip=True))
            if text: current_key = text; continue
        if bamei_td and not current_key:
            text = bamei_td.get_text(strip=True)
            if text: current_key = text; continue
        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current_key:
            danwa_dict[current_key] = danwa_td.get_text(strip=True)
            current_key = None
    return danwa_dict

def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°" in s)
    if not h2: return {}
    table = h2.find_next("table", class_="syoin")
    if not table or not table.tbody: return {}
    rows = table.tbody.find_all("tr")
    result_dict, i = {}, 0
    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []): i += 1; continue
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if not (uma_td and bamei_td): i += 1; continue
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True))
        name = bamei_td.get_text(strip=True)
        prev_comment = ""
        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼": prev_comment = txt
        if umaban: result_dict[umaban] = {"name": name, "prev_comment": prev_comment}
        i += 2
    return result_dict

def parse_cyokyo(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}
    section = None
    h2 = soup.find("h2", string=lambda s: s and ("èª¿æ•™" in s or "ä¸­é–“" in s))
    if h2:
        midasi = h2.find_parent("div", class_="midasi")
        if midasi: section = midasi.find_next_sibling("div", class_="section")
    if section is None: section = soup
    tables = section.find_all("table", class_="cyokyo")
    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody: continue
        rows = tbody.find_all("tr", recursive=False)
        if len(rows) < 1: continue
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")
        umaban = re.sub(r"\D", "", uma_td.get_text(strip=True)) if uma_td else ""
        bamei_hint = name_td.get_text(" ", strip=True) if name_td else ""
        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = detail_row.get_text(" ", strip=True) if detail_row else ""
        payload = {"tanpyo": tanpyo, "detail": detail_text, "bamei_hint": bamei_hint}
        if umaban: cyokyo_dict[umaban] = payload
        elif bamei_hint: cyokyo_dict[bamei_hint] = payload
    return cyokyo_dict

def parse_syutuba(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_=lambda c: c and "syutuba" in c)
    if not table or not table.tbody: return {}
    result = {}
    for tr in table.tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td", recursive=False)
        if not tds: continue
        umaban = re.sub(r"\D", "", tds[0].get_text(strip=True))
        if not umaban: continue
        bamei = ""
        kbamei_p = tr.find("p", class_="kbamei")
        if kbamei_p: bamei = kbamei_p.get_text(" ", strip=True)
        kisyu = ""
        kisyu_p = tr.find("p", class_="kisyu")
        if kisyu_p: kisyu = kisyu_p.get_text(strip=True)
        result[umaban] = {"umaban": umaban, "bamei": bamei, "kisyu": kisyu}
    return result

# ==================================================
# Netkeiba Scraper & è¿‘èµ°æŒ‡æ•°
# ==================================================
def fetch_netkeiba_data(driver, nk_race_id):
    """Netkeibaã®å‡ºé¦¬è¡¨(éå»èµ°)ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿‘èµ°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    url = f"https://race.netkeiba.com/race/shutuba_past.html?race_id={nk_race_id}&rf=shutuba_submenu"
    driver.get(url)
    time.sleep(1.0)
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    data_dict = {} # key: é¦¬å (Netkeibaã¯é¦¬ç•ªãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚é¦¬åãƒãƒƒãƒãƒ³ã‚°æ¨å¥¨ã ãŒã€ä»Šå›ã¯æš«å®šã§é¦¬å)
    
    # é¦¬ãƒªã‚¹ãƒˆã®å–å¾—
    rows = soup.select("tr.HorseList")
    for row in rows:
        # é¦¬åå–å¾—
        name_el = row.select_one(".HorseName a")
        if not name_el: continue
        horse_name = name_el.get_text(strip=True)
        
        # éå»èµ°ãƒ‡ãƒ¼ã‚¿ã®å–å¾— (æœ€å¤§5èµ°åˆ†)
        past_runs_html = row.select("td.Past")
        past_runs_data = []
        
        for run_td in past_runs_html:
            # é–‹å‚¬æ—¥ã€ãƒ¬ãƒ¼ã‚¹åã€ç€é †ã€é€šéé †ãªã©ã‚’å–å¾—
            data01 = run_td.select_one(".Data01")
            data02 = run_td.select_one(".Data02")
            data05 = run_td.select_one(".Data05") # é€šéé †ã¯ã“ã“ã«ã‚ã‚‹ã“ã¨ãŒå¤šã„
            
            if not (data01 and data02): continue
            
            # æ—¥ä»˜ãƒ»å ´æ‰€
            date_text = data01.get_text(" ", strip=True)
            # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼ˆé ­æ•°ã€æ ã€äººæ°—ãªã©ï¼‰
            race_meta = data02.get_text(" ", strip=True)
            
            # é€šéé †ã¨ç€é †ã®æŠ½å‡º
            passing_str = ""
            rank_str = ""
            
            # Data05ã‹ã‚‰é€šéé †ã‚’æ¢ã™ (ä¾‹: 10-10-7)
            if data05:
                passing_raw = data05.get_text(strip=True)
                # 7-11-13-13 ã®ã‚ˆã†ãªå½¢å¼ã‚’æŠ½å‡º
                match_pass = re.search(r'(\d+(?:-\d+)+)', passing_raw)
                if match_pass:
                    passing_str = match_pass.group(1)
            
            # ç€é †ã¯Data01ã®ä¸­ã«ã‚ã‚‹ã“ã¨ãŒå¤šã„ãŒã€æ§‹é€ ãŒè¤‡é›‘ãªãŸã‚Data01ã®æœ€åˆã®æ•°å­—ã‚„ã‚¯ãƒ©ã‚¹ã‚’ç¢ºèª
            # Netkeibaã®ã“ã®ãƒšãƒ¼ã‚¸ã¯ç€é †ãŒæ˜ç¤ºçš„ãªã‚¯ãƒ©ã‚¹(Rank)ã§æ›¸ã‹ã‚Œã¦ã„ã‚‹
            rank_el = run_td.select_one(".Rank")
            if rank_el:
                rank_str = rank_el.get_text(strip=True)
            
            if passing_str and rank_str:
                # æ•´å½¢: [2025.12.20 ... (7-11-13-13â†’6ç€)]
                # è©³ç´°ãªãƒ†ã‚­ã‚¹ãƒˆã¯ç°¡æ˜“åŒ–ã—ã¦çµåˆ
                full_text = f"[{date_text} {race_meta} ({passing_str}â†’{rank_str}ç€)]"
                past_runs_data.append({
                    "full_text": full_text,
                    "passing": passing_str,
                    "rank": rank_str
                })
                
        data_dict[horse_name] = past_runs_data
        
    return data_dict

def calculate_kinsou_index(past_runs_data):
    """
    è¿‘èµ°æŒ‡æ•°ã‚’è¨ˆç®—ã™ã‚‹
    â‘ è¿‘3èµ°ã®ã©ã‚Œã‹ã§ã€Œé“ä¸­é †ä½ãŒ4ã¤ä»¥ä¸Šæ‚ªåŒ–ã€ã‹ã¤ã€Œæœ€çµ‚ç€é †ãŒæœ€æ‚ªä½ç½®ã‚ˆã‚Š2ã¤ä»¥ä¸Šå·»ãè¿”ã—ã€ -> +8
    â‘¡è¿‘3èµ°ã®ã©ã‚Œã‹ã§ã€Œé“ä¸­é †ä½ãŒ2ã¤ä»¥ä¸Šæ‚ªåŒ–ã€ã‹ã¤ã€Œæœ€çµ‚ç€é †ãŒæœ€æ‚ªä½ç½®ã‚ˆã‚Š2ã¤ä»¥ä¸Šå·»ãè¿”ã—ã€ -> +5
    â‘¢è¿‘3èµ°ã®ã†ã¡50%ä»¥ä¸Šã§ã€Œ4ã‚³ãƒ¼ãƒŠãƒ¼ã®é †ä½ãŒ4ç•ªæ‰‹ä»¥å†…ã€ -> +2
    MAX 10ç‚¹
    """
    # è¿‘3èµ°ã«çµã‚‹
    recent_3 = past_runs_data[:3]
    if not recent_3:
        return 0.0
    
    base_score = 0
    corner4_ok_count = 0
    valid_runs = 0
    
    for run in recent_3:
        try:
            p_str = run["passing"]
            r_str = run["rank"]
            
            # é€šéé †ãƒªã‚¹ãƒˆåŒ– [7, 11, 13, 13]
            passes = [int(x) for x in p_str.split("-")]
            finish = int(re.sub(r"\D", "", r_str))
            
            valid_runs += 1
            
            # Rule 3 Check (4ã‚³ãƒ¼ãƒŠãƒ¼ <= 4)
            # é…åˆ—ã®æœ€å¾ŒãŒ4ã‚³ãƒ¼ãƒŠãƒ¼ã¨ä»®å®š
            if passes[-1] <= 4:
                corner4_ok_count += 1
            
            # Rule 1 & 2 Check
            # ã€Œé“ä¸­é †ä½ãŒæ‚ªåŒ–ã€: å§‹ç‚¹(ã¾ãŸã¯æœ€å°å€¤)ã¨æœ€æ‚ªå€¤(æœ€å¤§å€¤)ã®å·®ã¨å®šç¾©
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¾‹: 7-11-13-13 (7->13ã§6æ‚ªåŒ–)
            start_pos = passes[0]
            worst_pos = max(passes)
            worsened = worst_pos - start_pos
            
            # ã€Œå·»ãè¿”ã—ã€: æœ€æ‚ªå€¤ - ç€é †
            recovery = worst_pos - finish
            
            # åˆ¤å®š (ç‚¹æ•°ã®é«˜ã„æ–¹ã‚’å„ªå…ˆ)
            if worsened >= 4 and recovery >= 2:
                base_score = max(base_score, 8)
            elif worsened >= 2 and recovery >= 2:
                base_score = max(base_score, 5)
                
        except Exception:
            continue
            
    # Rule 3 Bonus
    bonus = 0
    if valid_runs > 0 and (corner4_ok_count / valid_runs) >= 0.5:
        bonus = 2
        
    total = base_score + bonus
    return min(float(total), 10.0)

# ==================================================
# Keibabook Fetch functions
# ==================================================
def fetch_danwa_dict(driver, race_id):
    driver.get(f"{BASE_URL}/cyuou/danwa/0/{race_id}")
    time.sleep(0.8)
    html = driver.page_source
    return html, parse_race_info(html), parse_danwa_comments(html)

def fetch_zenkoso_dict(driver, race_id):
    driver.get(f"{BASE_URL}/cyuou/syoin/{race_id}")
    time.sleep(0.8)
    return parse_zenkoso_interview(driver.page_source)

def fetch_cyokyo_dict(driver, race_id):
    driver.get(f"{BASE_URL}/cyuou/cyokyo/0/{race_id}")
    time.sleep(0.5)
    return parse_cyokyo(driver.page_source)

def fetch_syutuba_dict(driver, race_id):
    driver.get(f"{BASE_URL}/cyuou/syutuba/{race_id}")
    time.sleep(0.5)
    return parse_syutuba(driver.page_source)

# ==================================================
# Dify Streaming
# ==================================================
def stream_dify_workflow(full_text: str):
    if not DIFY_API_KEY:
        yield "âš ï¸ ã‚¨ãƒ©ãƒ¼: DIFY_API_KEY ãŒæœªè¨­å®š"
        return
    payload = {"inputs": {"text": full_text}, "response_mode": "streaming", "user": "keiba-bot-user"}
    headers = {"Authorization": f"Bearer {DIFY_API_KEY}", "Content-Type": "application/json"}
    try:
        res = requests.post("https://api.dify.ai/v1/workflows/run", headers=headers, json=payload, stream=True, timeout=300)
        if res.status_code != 200:
            yield f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {res.status_code}\n{res.text}"
            return
        for line in res.iter_lines():
            if not line: continue
            decoded = line.decode("utf-8", errors="ignore")
            if not decoded.startswith("data:"): continue
            json_str = decoded.replace("data: ", "")
            try:
                data = json.loads(json_str)
            except: continue
            if data.get("event") == "workflow_finished":
                outputs = data.get("data", {}).get("outputs", {})
                txt = "".join([v for v in outputs.values() if isinstance(v, str)])
                if txt: yield txt
            chunk = data.get("answer", "")
            if chunk: yield chunk
    except Exception as e:
        yield f"âš ï¸ Request Error: {str(e)}"

# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
def run_all_races(target_races=None):
    race_numbers = list(range(1, 13)) if target_races is None else sorted({int(r) for r in target_races})
    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_name = KB_PLACE_NAMES.get(PLACE, "ä¸æ˜")
    
    # Netkeiba IDã®æ§‹ç¯‰
    # KB: YYYY(4) KAI(2) PLACE(2) DAY(2) RR(2)
    # NK: YYYY(4) PLACE(2) KAI(2) DAY(2) RR(2)
    nk_place = KB_TO_NK_PLACE.get(PLACE, "06") # Default ä¸­å±±
    nk_base_id = f"{YEAR}{nk_place}{KAI}{DAY}"
    
    combined_blocks = []
    driver = build_driver()

    try:
        st.info("ğŸ”‘ ãƒ­ã‚°ã‚¤ãƒ³ä¸­...")
        login_keibabook(driver)
        st.success("âœ… ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")

        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num
            nk_race_id = nk_base_id + race_num
            
            st.markdown(f"### {place_name} {r}R")
            status_area = st.empty()
            result_area = st.empty()
            full_answer = ""

            try:
                status_area.info(f"ğŸ“¡ {place_name}{r}R ãƒ‡ãƒ¼ã‚¿åé›†ä¸­ (KB & Netkeiba)...")
                
                # 1. Keibabook Data
                _html, race_info, danwa_dict = fetch_danwa_dict(driver, race_id)
                zenkoso_dict = fetch_zenkoso_dict(driver, race_id)
                cyokyo_dict = fetch_cyokyo_dict(driver, race_id)
                syutuba_dict = fetch_syutuba_dict(driver, race_id)
                
                # 2. Netkeiba Data (è¿‘èµ°)
                nk_data = fetch_netkeiba_data(driver, nk_race_id)

                merged = []
                umaban_list = sorted(syutuba_dict.keys(), key=lambda x: int(x)) if syutuba_dict else []

                for umaban in umaban_list:
                    sb = syutuba_dict.get(umaban, {})
                    bamei = (sb.get("bamei") or "").strip()
                    kisyu = sb.get("kisyu", "ä¸æ˜")
                    
                    # å©èˆ
                    d_cmt = danwa_dict.get(umaban) or danwa_dict.get(bamei) or "ï¼ˆæƒ…å ±ãªã—ï¼‰"
                    
                    # Netkeiba è¿‘èµ°ãƒ‡ãƒ¼ã‚¿ & æŒ‡æ•°è¨ˆç®—
                    nk_horse_data = nk_data.get(bamei, [])
                    kinsou_score = calculate_kinsou_index(nk_horse_data)
                    
                    # è¿‘èµ°æ–‡å­—åˆ—ã®ä½œæˆ
                    kinsou_text_list = [d["full_text"] for d in nk_horse_data]
                    kinsou_block_str = " / ".join(kinsou_text_list) if kinsou_text_list else "ï¼ˆæƒ…å ±ãªã—ï¼‰"

                    # å‰èµ°ï¼ˆKeibabookã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼‰
                    z_data = zenkoso_dict.get(umaban) or zenkoso_dict.get(bamei) or {}
                    z_comment = z_data.get("prev_comment", "ï¼ˆç„¡ã—ï¼‰")

                    # èª¿æ•™
                    c = cyokyo_dict.get(umaban) or cyokyo_dict.get(bamei) or {}
                    c_str = f"çŸ­è©•:{c.get('tanpyo','')} / è©³ç´°:{c.get('detail','')}"

                    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
                    # ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãªã©ã¯ç¾çŠ¶è¨ˆç®—å…ƒãŒãªã„ãŸã‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¾ãŸã¯Difyå´æ¨è«–ã«ä»»ã›ã‚‹å‰æã§æ ã®ã¿ä½œæˆ
                    # è¿‘èµ°æŒ‡æ•°ã®ã¿Pythonã§è¨ˆç®—ã—ãŸå€¤ã‚’åŸ‹ã‚è¾¼ã‚€
                    text = (
                        f"â–¼{syutuba_dict.get(umaban,{}).get('waku','?')}æ {umaban}ç•ª {bamei} (é¨æ‰‹:{kisyu})\n"
                        f"ã€ãƒ‡ãƒ¼ã‚¿ã€‘ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°:-- (åå·®å€¤:--) ãƒã‚¤ã‚¢ã‚¹:-- è¿‘èµ°æŒ‡æ•°:{kinsou_score:.1f}/10 F:--\n"
                        f"ã€å©èˆã€‘{d_cmt}\n"
                        f"ã€å‰èµ°è«‡è©±ã€‘{z_comment}\n"
                        f"ã€èª¿æ•™ã€‘{c_str}\n"
                        f"ã€è¿‘èµ°ã€‘{kinsou_block_str}\n"
                    )
                    merged.append(text)

                # Output Generation
                header_txt = "\n".join([v for v in race_info.values() if v])
                full_text = (
                    "â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n" + header_txt + "\n\n"
                    f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã€‚\n"
                    "â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n" + "\n".join(merged)
                )

                status_area.info("ğŸ¤– AIåˆ†æä¸­...")
                for chunk in stream_dify_workflow(full_text):
                    if chunk:
                        full_answer += chunk
                        result_area.markdown(full_answer + "â–Œ")
                
                result_area.markdown(full_answer)
                if full_answer:
                    status_area.success("âœ… å®Œäº†")
                    save_history(YEAR, KAI, PLACE, place_name, DAY, race_num, race_id, full_answer)
                    combined_blocks.append(f"ã€{place_name} {r}Rã€‘\n{full_answer.strip()}\n")
                    
                    # Copy Button
                    dom_id = f"copy_{race_id}_{int(time.time())}"
                    render_copy_button(full_answer.strip(), f"ğŸ“‹ {r}R ã‚³ãƒ”ãƒ¼", dom_id)

            except Exception as e:
                st.error(f"Error {r}R: {e}")

        # Summary
        if combined_blocks:
            final_txt = "\n".join(combined_blocks)
            st.subheader("ğŸ“Œ å…¨ãƒ¬ãƒ¼ã‚¹ã¾ã¨ã‚")
            render_copy_button(final_txt, "ğŸ“‹ å…¨æ–‡ã‚³ãƒ”ãƒ¼", "copy_all_final")
            st.download_button("â¬‡ï¸ TXTä¿å­˜", final_txt, file_name=f"KEIBA_{place_name}_ALL.txt")

    finally:
        driver.quit()

# ==================================================
# UI Entry Point
# ==================================================
st.title("ğŸ‡ AIç«¶é¦¬äºˆæƒ³ (KBÃ—Netkeiba è¿‘èµ°æŒ‡æ•°ç‰ˆ)")

with st.sidebar:
    st.header("é–‹å‚¬è¨­å®š")
    y = st.text_input("å¹´", YEAR)
    k = st.text_input("å›", KAI)
    p = st.selectbox("å ´æ‰€", list(KB_PLACE_NAMES.keys()), index=5, format_func=lambda x: KB_PLACE_NAMES[x])
    d = st.text_input("æ—¥", DAY)
    
    if st.button("è¨­å®šåæ˜ "):
        set_race_params(y, k, p, d)
        st.success(f"è¨­å®š: {y}å¹´ {k}å› {KB_PLACE_NAMES[p]} {d}æ—¥ç›®")

    st.markdown("---")
    st.markdown("### å¯¾è±¡ãƒ¬ãƒ¼ã‚¹å®Ÿè¡Œ")
    if st.button("å…¨ãƒ¬ãƒ¼ã‚¹ (1-12R)"):
        run_all_races()
    
    st.markdown("---")
    r_single = st.number_input("å˜ä¸€ãƒ¬ãƒ¼ã‚¹å®Ÿè¡Œ", 1, 12, 11)
    if st.button(f"{r_single}R ã®ã¿å®Ÿè¡Œ"):
        run_all_races([r_single])
