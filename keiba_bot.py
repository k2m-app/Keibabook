import time
import requests
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ==================================================
# ã€è¨­å®šã‚¨ãƒªã‚¢ã€‘
# ==================================================

KEIBA_ID = st.secrets.get("KEIBA_ID", "")
KEIBA_PASS = st.secrets.get("KEIBA_PASS", "")
DIFY_API_KEY = st.secrets.get("DIFY_API_KEY", "")

SUPABASE_URL = st.secrets.get("SUPABASE_URL", "")
SUPABASE_ANON_KEY = st.secrets.get("SUPABASE_ANON_KEY", "")

YEAR = "2025"
KAI = "04"
PLACE = "02"
DAY = "02"


def set_race_params(year, kai, place, day):
    global YEAR, KAI, PLACE, DAY
    YEAR = str(year)
    KAI = str(kai).zfill(2)
    PLACE = str(place).zfill(2)
    DAY = str(day).zfill(2)


# ==================================================
# Supabase
# ==================================================
@st.cache_resource
def get_supabase_client() -> Client:
    if not SUPABASE_URL or not SUPABASE_ANON_KEY:
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def save_history(year, kai, place_code, place_name, day, race_num_str, race_id, ai_answer):
    supabase = get_supabase_client()
    if supabase is None:
        print("âš  Supabase æœªè¨­å®š â†’ ä¿å­˜ã—ã¾ã›ã‚“")
        return

    data = {
        "year": str(year),
        "kai": str(kai),
        "place_code": str(place_code),
        "place_name": place_name,
        "day": str(day),
        "race_num": race_num_str,
        "race_id": race_id,
        "output_text": ai_answer,
    }

    try:
        supabase.table("history").insert(data).execute()
        print("ğŸ’¾ å±¥æ­´ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print("âš  å±¥æ­´ä¿å­˜å¤±æ•—:", e)


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ¬ãƒ¼ã‚¹æƒ…å ±ï¼šè·é›¢ãƒ»ã‚³ãƒ¼ã‚¹ãƒ»é¦¬å ´ãªã©ï¼‰
# ==================================================
def parse_race_info(html: str):
    """
    å©èˆã®è©±ãƒšãƒ¼ã‚¸ã® HTML ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’æŠœãå‡ºã™ã€‚
    ä¾‹ï¼š
      2025å¹´12æœˆ7æ—¥ 5å›ä¸­äº¬2æ—¥ç›®
      1R ï¼’æ­³æœªå‹åˆ©
      [æŒ‡å®š],(æ··)
      1800m (ãƒ€ãƒ¼ãƒˆãƒ»å·¦) æ™´ãƒ»è‰¯ å¹³ã€€ã€€ç©
    """
    soup = BeautifulSoup(html, "html.parser")
    racetitle = soup.find("div", class_="racetitle")
    if not racetitle:
        return {
            "date_meet": "",
            "race_name": "",
            "cond1": "",
            "course_line": "",
        }

    # ä¸Šæ®µï¼ˆé–‹å‚¬æ—¥ï¼‹ãƒ¬ãƒ¼ã‚¹åï¼‰
    racemei = racetitle.find("div", class_="racemei")
    date_meet = ""
    race_name = ""
    if racemei:
        ps = racemei.find_all("p")
        if len(ps) >= 1:
            date_meet = ps[0].get_text(strip=True)
        if len(ps) >= 2:
            race_name = ps[1].get_text(strip=True)

    # ä¸‹æ®µï¼ˆ[æŒ‡å®š],(æ··) ï¼ è·é›¢ãƒ»ã‚³ãƒ¼ã‚¹ãƒ»å¤©å€™ãƒ»é¦¬å ´ãªã©ï¼‰
    racetitle_sub = racetitle.find("div", class_="racetitle_sub")
    cond1 = ""
    course_line = ""
    if racetitle_sub:
        sub_ps = racetitle_sub.find_all("p")
        if len(sub_ps) >= 1:
            cond1 = sub_ps[0].get_text(strip=True)  # [æŒ‡å®š],(æ··) ãªã©
        if len(sub_ps) >= 2:
            # "1800m (ãƒ€ãƒ¼ãƒˆãƒ»å·¦) æ™´ãƒ»è‰¯ å¹³ã€€ã€€ç©" ã®è¡Œ
            course_line = sub_ps[1].get_text(" ", strip=True)

    return {
        "date_meet": date_meet,
        "race_name": race_name,
        "cond1": cond1,
        "course_line": course_line,
    }


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹ï¼ˆå‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼‰
# ==================================================
def parse_zenkoso_interview(html: str):
    soup = BeautifulSoup(html, "html.parser")
    h2 = soup.find("h2", string=lambda s: s and "å‰èµ°ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" in s)
    if not h2:
        return []

    midasi = h2.find_parent("div", class_="midasi")
    table = midasi.find_next("table", class_="syoin")
    if not table or not table.tbody:
        return []

    rows = table.tbody.find_all("tr")
    result = []
    i = 0

    while i < len(rows):
        row = rows[i]
        if "spacer" in (row.get("class") or []):
            i += 1
            continue

        waku_td = row.find("td", class_="waku")
        uma_td = row.find("td", class_="umaban")
        bamei_td = row.find("td", class_="bamei")
        if not (waku_td and uma_td and bamei_td):
            i += 1
            continue

        waku = waku_td.get_text(strip=True)
        umaban = uma_td.get_text(strip=True)
        name = bamei_td.get_text(strip=True)

        prev_date = ""
        prev_class = ""
        prev_finish = ""
        prev_comment = ""

        detail = rows[i + 1] if i + 1 < len(rows) else None
        if detail:
            syoin_td = detail.find("td", class_="syoin")
            if syoin_td:
                sdata = syoin_td.find("div", class_="syoindata")
                if sdata:
                    ps = sdata.find_all("p")
                    if ps:
                        prev_date = ps[0].get_text(strip=True)
                    if len(ps) >= 2:
                        spans = ps[1].find_all("span")
                        if len(spans) >= 1:
                            prev_class = spans[0].get_text(strip=True)
                        if len(spans) >= 2:
                            prev_finish = spans[1].get_text(strip=True)

                direct = syoin_td.find_all("p", recursive=False)
                if direct:
                    txt = direct[0].get_text(strip=True)
                    if txt != "ï¼":
                        prev_comment = txt

        result.append({
            "waku": waku,
            "umaban": umaban,
            "name": name,
            "prev_date_course": prev_date,
            "prev_class": prev_class,
            "prev_finish": prev_finish,
            "prev_comment": prev_comment,
        })

        i += 2

    return result


# ==================================================
# HTML ãƒ‘ãƒ¼ã‚¹ï¼ˆå©èˆã®è©±ï¼‰
# ==================================================
def parse_danwa_comments(html: str):
    soup = BeautifulSoup(html, "html.parser")
    table = soup.find("table", class_="danwa")
    if not table:
        return {}

    danwa_dict = {}
    current = None

    for row in table.tbody.find_all("tr"):
        uma_td = row.find("td", class_="umaban")
        if uma_td:
            current = uma_td.get_text(strip=True)
            continue

        danwa_td = row.find("td", class_="danwa")
        if danwa_td and current:
            danwa_dict[current] = danwa_td.get_text(strip=True)
            current = None

    return danwa_dict


# ==================================================
# â˜… èª¿æ•™ãƒšãƒ¼ã‚¸ ãƒ‘ãƒ¼ã‚¹ï¼ˆå®Œå…¨ç‰ˆï¼‰
# ==================================================
def parse_cyokyo(html: str):
    """
    èª¿æ•™ãƒšãƒ¼ã‚¸ã®HTMLã‹ã‚‰
    { "é¦¬ç•ª": "æ•´å½¢æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ" } ã®dictã‚’è¿”ã™
    """
    soup = BeautifulSoup(html, "html.parser")
    cyokyo_dict = {}

    # â‘  ã€Œèª¿æ•™ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
    section = None
    h2 = soup.find("h2", string=lambda s: s and "èª¿æ•™" in s)
    if h2:
        midasi_div = h2.find_parent("div", class_="midasi")
        if midasi_div:
            section = midasi_div.find_next_sibling("div", class_="section")

    if section is None:
        # å¿µã®ãŸã‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        section = soup

    # â‘¡ section å†…ã® table.cyokyo ã‚’ã™ã¹ã¦åˆ—æŒ™
    tables = section.find_all("table", class_="cyokyo")
    print("â˜…DEBUG table.cyokyo å€‹æ•°:", len(tables))

    for tbl in tables:
        tbody = tbl.find("tbody")
        if not tbody:
            continue

        rows = tbody.find_all("tr", recursive=False)
        if not rows:
            continue

        # 1è¡Œç›® = ãƒ˜ãƒƒãƒ€ï¼ˆæ ç•ªãƒ»é¦¬ç•ªãƒ»é¦¬åãƒ»çŸ­è©•ãƒ»çŸ¢å°ï¼‰
        header = rows[0]
        uma_td = header.find("td", class_="umaban")
        name_td = header.find("td", class_="kbamei")

        # é¦¬ç•ªãƒ»é¦¬åãŒå–ã‚Œãªã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
        if not uma_td or not name_td:
            continue

        umaban = uma_td.get_text(strip=True)
        bamei = name_td.get_text(" ", strip=True)

        tanpyo_td = header.find("td", class_="tanpyo")
        tanpyo = tanpyo_td.get_text(strip=True) if tanpyo_td else ""

        # 2è¡Œç›® = èª¿æ•™è©³ç´°ï¼ˆdl + table.cyokyodata ç­‰ï¼‰
        detail_row = rows[1] if len(rows) >= 2 else None
        detail_text = ""
        if detail_row:
            # è¡Œå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã™ã¹ã¦ç©ºç™½åŒºåˆ‡ã‚Šã§é€£çµ
            detail_text = detail_row.get_text(" ", strip=True)

        # å¥½ããªå½¢ã«æ•´å½¢
        final_text = f"ã€é¦¬åã€‘{bamei}ï¼ˆé¦¬ç•ª{umaban}ï¼‰ ã€çŸ­è©•ã€‘{tanpyo} ã€èª¿æ•™è©³ç´°ã€‘{detail_text}"
        cyokyo_dict[umaban] = final_text

    print("â˜…DEBUG èª¿æ•™ dict keys:", list(cyokyo_dict.keys()))
    return cyokyo_dict


# ==================================================
# èª¿æ•™å–å¾—é–¢æ•°
# ==================================================
BASE_URL = "https://s.keibabook.co.jp"


def fetch_cyokyo_dict(driver, race_id: str):
    url = f"{BASE_URL}/cyuou/cyokyo/0/{race_id}"
    print("â˜…DEBUG èª¿æ•™ãƒšãƒ¼ã‚¸ URL:", url)
    driver.get(url)

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "table.cyokyo"))
        )
    except Exception as e:
        print("âŒ WebDriverWaitã§ table.cyokyo ãŒè¦‹ã¤ã‹ã‚‰ãš:", e)
        print("current_url:", driver.current_url)
        src_head = driver.page_source[:2000]
        print("page_source å†’é ­(2000æ–‡å­—):\n", src_head)
        return {}

    html = driver.page_source
    cy = parse_cyokyo(html)

    print("â˜…DEBUG èª¿æ•™:", cy)
    return cy


# ==================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==================================================
def run_all_races(target_races=None):

    race_numbers = (
        list(range(1, 13))
        if target_races is None
        else sorted({int(r) for r in target_races})
    )

    base_id = f"{YEAR}{KAI}{PLACE}{DAY}"
    place_names = {
        "00": "äº¬éƒ½", "01": "é˜ªç¥", "02": "ä¸­äº¬", "03": "å°å€‰",
        "04": "æ±äº¬", "05": "ä¸­å±±", "06": "ç¦å³¶", "07": "æ–°æ½Ÿ",
        "08": "æœ­å¹Œ", "09": "å‡½é¤¨",
    }
    place_name = place_names.get(PLACE, "ä¸æ˜")

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)

    try:
        # ãƒ­ã‚°ã‚¤ãƒ³
        driver.get("https://s.keibabook.co.jp/login/login")

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.NAME, "login_id"))
        ).send_keys(KEIBA_ID)

        WebDriverWait(driver, 10).until(
            EC.visibility_of_element_located((By.CSS_SELECTOR, "input[type='password']"))
        ).send_keys(KEIBA_PASS)

        WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "input[type='submit'], .btn-login"))
        ).click()

        time.sleep(2)

        # å„Rå‡¦ç†
        for r in race_numbers:
            race_num = f"{r:02}"
            race_id = base_id + race_num

            print(f"\n=== {place_name} {r}R ({race_id}) ===")

            # å©èˆã‚³ãƒ¡ãƒ³ãƒˆï¼ˆï¼‹ã“ã“ã§ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚‚å–ã‚‹ï¼‰
            url_danwa = f"https://s.keibabook.co.jp/cyuou/danwa/0/{race_id}"
            print("â˜…DEBUG å©èˆã‚³ãƒ¡ãƒ³ãƒˆ URL:", url_danwa)
            driver.get(url_danwa)
            time.sleep(1)

            html_danwa = driver.page_source
            race_info = parse_race_info(html_danwa)
            danwa_dict = parse_danwa_comments(html_danwa)
            print("â˜…DEBUG å©èˆã‚³ãƒ¡ãƒ³ãƒˆ dict keys:", list(danwa_dict.keys()))
            print("â˜…DEBUG ãƒ¬ãƒ¼ã‚¹æƒ…å ±:", race_info)

            # å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
            url_inter = f"https://s.keibabook.co.jp/cyuou/syoin/{race_id}"
            print("â˜…DEBUG å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ URL:", url_inter)
            driver.get(url_inter)
            time.sleep(1)
            zenkoso = parse_zenkoso_interview(driver.page_source)
            print("â˜…DEBUG å‰èµ°ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼é ­æ•°:", len(zenkoso))

            # èª¿æ•™
            cyokyo_dict = fetch_cyokyo_dict(driver, race_id)

            # é¦¬ã”ã¨ã«ãƒãƒ¼ã‚¸
            merged = []
            for h in zenkoso:
                uma = h["umaban"]
                text = (
                    f"â–¼[æ {h['waku']} é¦¬ç•ª{uma}] {h['name']}\n"
                    f"  ã€å©èˆã®è©±ã€‘ {danwa_dict.get(uma, 'ï¼ˆå©èˆã‚³ãƒ¡ãƒ³ãƒˆãªã—ï¼‰')}\n"
                    f"  ã€å‰èµ°æƒ…å ±ã€‘ {h['prev_date_course']} ({h['prev_class']}) {h['prev_finish']}\n"
                    f"  ã€å‰èµ°è«‡è©±ã€‘ {h['prev_comment'] or 'ï¼ˆå‰èµ°è«‡è©±ãªã—ï¼‰'}\n"
                    f"  ã€èª¿æ•™ã€‘ {cyokyo_dict.get(uma, 'ï¼ˆèª¿æ•™æƒ…å ±ãªã—ï¼‰')}\n"
                )
                merged.append(text)

            # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢
            race_header_lines = []
            if race_info["date_meet"]:
                race_header_lines.append(race_info["date_meet"])
            if race_info["race_name"]:
                race_header_lines.append(race_info["race_name"])
            if race_info["cond1"]:
                race_header_lines.append(race_info["cond1"])
            if race_info["course_line"]:
                # ã“ã“ã«ã€Œ1800m (ãƒ€ãƒ¼ãƒˆãƒ»å·¦) æ™´ãƒ»è‰¯ å¹³ã€€ã€€ç©ã€ãŒå…¥ã‚‹
                race_header_lines.append(race_info["course_line"])

            race_header = "\n".join(race_header_lines)

            full_text = (
                f"â– ãƒ¬ãƒ¼ã‚¹æƒ…å ±\n"
                f"{race_header}\n\n"
                f"ä»¥ä¸‹ã¯{place_name}{r}Rã®å…¨é ­ãƒ‡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚"
                f"å„é¦¬ã«ã¤ã„ã¦ã€å©èˆã®è©±ã€‘ã€å‰èµ°æƒ…å ±ãƒ»å‰èµ°è«‡è©±ã€‘ã€èª¿æ•™ã€‘ã‚’åŸºã«åˆ†æã›ã‚ˆã€‚\n\n"
                f"â– å‡ºèµ°é¦¬è©³ç´°ãƒ‡ãƒ¼ã‚¿\n" +
                "\n".join(merged)
            )

            payload = {
                "inputs": {"text": full_text},
                "response_mode": "blocking",
                "user": "keiba-bot-user",
            }

            headers = {
                "Authorization": f"Bearer {DIFY_API_KEY}",
                "Content-Type": "application/json",
            }

            res = requests.post(
                "https://api.dify.ai/v1/workflows/run",
                headers=headers,
                json=payload
            )

            if res.status_code == 200:
                ans = res.json().get("data", {}).get("outputs", {}).get("answer", "")
                st.markdown(f"### {place_name} {r}R")
                st.write(ans)
                st.write("---")

                save_history(YEAR, KAI, PLACE, place_name, DAY,
                             race_num, race_id, ans)

            else:
                print("âŒ Dify ã‚¨ãƒ©ãƒ¼:", res.status_code, res.text)

    finally:
        print("\nğŸ§¹ ãƒ–ãƒ©ã‚¦ã‚¶çµ‚äº†")
        driver.quit()
